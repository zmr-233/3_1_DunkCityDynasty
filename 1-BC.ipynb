{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-BC阶段\n",
    "\n",
    "使用`4_DCD380`环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔵nn.Sequential 容器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一共有两种形式，接收两种参数：\n",
    "\n",
    "#### 🔹`torch.nn.Sequential(*args: Module)`\n",
    "\n",
    "#### 🔹`torch.nn.Sequential(arg: OrderedDict[str, Module])`\n",
    "一个顺序容器。模块将按照它们在构造函数中传递的顺序添加到其中。另外，也可以传入一个模块的有序字典。`Sequential` 的 `forward()` 方法接受任何输入，并将其转发到其包含的第一个模块。然后它将输出“链式”连接到每个后续模块的输入，最终返回最后一个模块的输出。\n",
    "\n",
    "与手动调用一系列模块相比，`Sequential` 的价值在于它允许将整个容器视为单个模块，这样对 `Sequential` 进行的转换将应用于其存储的每个模块（每个模块都是 `Sequential` 的一个注册子模块）。\n",
    "\n",
    "`Sequential` 和 `torch.nn.ModuleList` 之间有什么区别？`ModuleList` 正如其名——用于存储模块的列表！另一方面，`Sequential` 中的层以级联方式连接。\n",
    "```python\n",
    "# 使用Sequential创建一个小模型。当运行`model`时，\n",
    "# 输入首先会传递给`Conv2d(1,20,5)`。`Conv2d(1,20,5)`的输出\n",
    "# 将用作第一个`ReLU`的输入；第一个`ReLU`的输出将成为\n",
    "# `Conv2d(20,64,5)`的输入。最后，`Conv2d(20,64,5)`的输出\n",
    "# 将用作第二个`ReLU`的输入\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "# 使用有序字典与Sequential。这在功能上与上述代码相同\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "```\n",
    "##### 🔺`.append(module)`将给定模块追加到末尾\n",
    "用法比较简单，就是追加模块的：\n",
    "```python\n",
    "model.append(nn.Module)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔵collections.defaultdict 默认值字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它是一个类似于常规字典的容器，但它有一个特点：当你试图访问一个不存在的键时，它可以自动为该键提供一个默认值。这个默认值是由你提供的工厂函数生成的，意味着不可能会发生\"键值异常\"：\n",
    "\n",
    "只有一种形式，接收两个参数：\n",
    "#### 🔹`class collections.defaultdict(default_factory=None, /[, ...])`\n",
    "\n",
    "##### @param `__missing__(key)`\n",
    "1. 如果 default_factory 属性为 None，则调用本方法会抛出 KeyError 异常，附带参数 key。\n",
    "2. 如果 default_factory 不为 None，则它会被（不带参数地）调用来为 key 提供一个默认值，这个值和 key 作为一对键值对被插入到字典中，并作为本方法的返回值返回\n",
    "\n",
    "##### @param `default_factory=`\n",
    "本属性由 __missing__() 方法来调用。如果构造对象时提供了第一个参数，则本属性会被初始化成那个参数，如果未提供第一个参数，则本属性为 None。\n",
    "\n",
    "##### 示例：\n",
    "1. default_factory是一个类型\n",
    "```python\n",
    "ep_rewards = defaultdict(int)\n",
    "ep_rewards[\"hhh\"] +=1 #当hhh键不存在，则自动设置0\n",
    "\n",
    "d = defaultdict(list)\n",
    "d[\"hhh\"].append(1) #自动生成一个空列表\n",
    "```\n",
    "\n",
    "2. 使用lambda函数，可以提供任何常量值\n",
    "```python\n",
    "def constant_factory(value):\n",
    "    return lambda: value\n",
    "\n",
    "d = defaultdict(constant_factory('<missing>'))\n",
    "d.update(name='John', action='ran')\n",
    "'%(name)s %(action)s to %(object)s' % d\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔵collections.deque\n",
    "```\n",
    "from collections import deque\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deque 支持线程安全，内存高效添加(append)和弹出(pop)，从两端都可以，两个方向的大概开销都是 O(1) 复杂度。 索引访问在两端的复杂度均为 O(1) 但在中间则会低至 O(n)，如果需要快速访问，请改用列表\n",
    "\n",
    "#### 🔹`class collections.deque([iterable[, maxlen]])`\n",
    "如果 maxlen 没有指定或者是 None ，deques 可以增长到任意长度。\n",
    "一旦限定长度的deque满了，当新项加入时，同样数量的项就从另一端弹出。因此可以起到一个数据交换池的作用。\n",
    "\n",
    "提供一个只读属性：\n",
    "- **_@attr `maxlen`_**<br>\n",
    "Deque的最大尺寸，如果没有限定的话就是 None\n",
    "\n",
    "提供方法如下：\n",
    "- **_@method `append(x)`_**<br>\n",
    "添加 x 到右端\n",
    "- **_@method `appendleft(x)`_**<br>\n",
    "添加 x 到左端\n",
    "- **_@method `clear()`_**<br>\n",
    "移除所有元素，使其长度为0\n",
    "- **_@method `copy()`_**<br>\n",
    "创建一份浅拷贝\n",
    "- **_@method `count(x)`_**<br>\n",
    "计算 deque 中元素等于 x 的个数\n",
    "- **_@method `extend(iterable)`_**<br>\n",
    "扩展deque的右侧，通过添加iterable参数中的元素\n",
    "- **_@method `extendleft(iterable)`_**<br>\n",
    "扩展deque的左侧，通过添加iterable参数中的元素。注意，左添加时，在结果中iterable参数中的顺序将被反过来添加。\n",
    "- **_@method `index(x[, start[, stop]])`_**<br>\n",
    "返回 x 在 deque 中的位置（在索引 start 之后，索引 stop 之前）。 返回第一个匹配项，如果未找到则引发 ValueError\n",
    "- **_@method `insert(i, x)`_**<br>\n",
    "在位置 i 插入 x<br>如果插入会导致一个限长 deque 超出长度 maxlen 的话，就引发一个 IndexError\n",
    "- **_@method `pop()`_**<br>\n",
    "移去并且返回一个元素，deque 最右侧的那一个。 如果没有元素的话，就引发一个 IndexError\n",
    "- **_@method `popleft()`_**<br>\n",
    "移去并且返回一个元素，deque 最左侧的那一个。 如果没有元素的话，就引发 IndexError\n",
    "- **_@method `remove(value)`_**<br>\n",
    "移除找到的第一个 value。 如果没有的话就引发 ValueError\n",
    "- **_@method `reverse()`_**<br>\n",
    "将deque逆序排列。返回 None\n",
    "- **_@method `rotate(n=1)`_**<br>\n",
    "向右循环移动 n 步。 如果 n 是负数，就向左循环<br>\n",
    "如果deque不是空的，向右循环移动一步就等价于 d.appendleft(d.pop()) ， 向左循环一步就等价于 d.append(d.popleft())\n",
    "\n",
    "其他支持方法如下：\n",
    "\n",
    "deque 还支持迭代、封存、len(d)、reversed(d)、copy.copy(d)、copy.deepcopy(d)、成员检测运算符 in 以及下标引用:\n",
    "- `d[0]`访问首个元素，`d[-1]`访问最后一个元素\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deque用法\n",
    "\n",
    "1. 限长deque提供了类似于Unix tail的过滤功能\n",
    "2. 一个`轮询调度器`可以通过在 deque 中放入迭代器来实现\n",
    "3. rotate() 方法提供了一种方式来实现 deque 切片和删除<br>\n",
    "    例如， 一个纯的Python del d[n] 实现依赖于 rotate() 来定位要弹出的元素\n",
    "    ```python\n",
    "    def delete_nth(d, n):\n",
    "        d.rotate(-n)\n",
    "        d.popleft()\n",
    "        d.rotate(n)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[其余详细用法见官方手册](https://docs.python.org/zh-cn/3/library/collections.html#defaultdict-objects)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔵内置函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🔺动态设置对象属性|@F|`setattr(object, name, value)`\n",
    "动态设置对象属性,如`setattr(x, 'foobar', 123)`等价于`x.foobar = 123`\n",
    "- **_@param `object`_**<br>\n",
    "对象:要设置属性的对象，通常是`self`，\n",
    "- **_@param `name(str)`_**<br>\n",
    "属性名:设置的属性的名称(字符串)\n",
    "- **_@param `value`_**<br>\n",
    "属性值:设置的属性的值\n",
    "\n",
    "用法举例:\n",
    "<br>假设`kwargs`是一个字典，则可以用setattr直接把key变为对象属性\n",
    "```python\n",
    "def __init__(self, **kwargs) -> None:\n",
    "    for k,v in kwargs.items():\n",
    "        setattr(self,k,v)\n",
    "```\n",
    "---\n",
    "\n",
    "##### 🔺动态获取对象属性|@F|`getattr(object, name, default(opt))`\n",
    "该功能类似于Java的反射，可以把对象的属性当做字典那样用字符串获取\n",
    "- **_@param `object`_**<br>\n",
    "对象:你想获取其属性的对象，\n",
    "- **_@param `name(str)`_**<br>\n",
    "属性名:你想获取的属性的名称(字符串)\n",
    "- **_@param `default(opt)s`_**<br>\n",
    "默认值(可选):如果指定的属性不存在，将返回此值\n",
    "\n",
    "用法举例：\n",
    "```python\n",
    "class MyClass:\n",
    "    attribute = \"Default Value\"\n",
    "obj - MyClass()\n",
    "\n",
    "value = getattr(obj, 'attribute')\n",
    "print(value)  # 输出: Default Value\n",
    "\n",
    "# 获取一个不存在的属性，如果不存在，则返回指定的默认值 \"Not Found\"\n",
    "value_not_found = getattr(obj, 'non_existent_attr', \"Not Found\")\n",
    "print(value_not_found)  # 输出: Not Found\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔵torch相关函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 🔺指定层数升维|pt@F|`torch.unsqueeze(input, dim)`\n",
    "该函数起升维的作用,参数dim表示在哪个地方加一个维度\n",
    "- **_@param `input`_**<br>\n",
    "原始输入\n",
    "- **_@param `dim`_**<br>\n",
    "注意dim范围在:$[-input.dim() - 1, input.dim() + 1]$\n",
    "\n",
    "代码举例：\n",
    "```python\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "> print(torch.unsqueeze(x, 0))\n",
    "tensor([[1, 2, 3, 4]])\n",
    "> print(torch.unsqueeze(x, 1))\n",
    "tensor([[1],\n",
    "        [2],\n",
    "        [3],\n",
    "        [4]])\n",
    "```\n",
    "\n",
    "形象理解：<br>\n",
    "> 要理解为嵌套的层数，而不要从维度去理解，所谓dim=0就是最外层括号，dim=1就是内层括号，dim=2就是最内层括号\n",
    "[Stack Overflow 形象理解](https://img-blog.csdnimg.cn/179532e51a3a4c749c0073f4d0cbd861.png)\n",
    "\n",
    "---\n",
    "\n",
    "##### 🔺维度压缩|pt@F|`torch.unsqueeze(input, dim)`\n",
    "功能是维度压缩。返回一个tensor（张量），其中 input 中维度大小为1的所有维都已删除。<br>\n",
    "> 举个例子：如果 input 的形状为 (A×1×B×C×1×D)，那么返回的tensor的形状则为 (A×B×C×D)\n",
    "\n",
    "**当给定 dim 时，那么只在给定的维度（dimension）上进行压缩操作，注意给定的维度大小必须是1，否则不能进行压缩!**\n",
    "> 举个例子：如果 input 的形状为 (A×1×B)，squeeze(input, dim=0)后，返回的tensor不变，因为第0维的大小为A，不是1；squeeze(input, 1)后，返回的tensor将被压缩为 (A×B)。\n",
    "- **_@param `input`_**<br>\n",
    "原始输入\n",
    "- **_@param `dim`_**<br>\n",
    "要删除的维度，注意给定的dim的大小必须为1，比如当batch_size = 1要还原的时候\n",
    "\n",
    "代码举例：\n",
    "```python\n",
    "x = torch.randn(size=(2, 1, 2, 1, 2))\n",
    "#shape: torch.Size([2, 1, 2, 1, 2])\n",
    "torch.squeeze(x)\n",
    "#shape: torch.Size([2, 2, 2])\n",
    "\n",
    "#把x中第一维删除，但是第一维大小为2，不为1，因此结果删除不掉\n",
    "torch.squeeze(x,dim=-1)\n",
    "#shape: torch.Size([2, 1, 2, 1, 2])\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States状态shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### json> state_infos\n",
    "```python\n",
    "#---->state_infos:(json)\n",
    "{\n",
    "    #for key in state_infos.keys():\n",
    "    \"0\":[\n",
    "        #infos[key]:\n",
    "        #---->infos[key] = state_infos[key][0]\n",
    "        {\n",
    "            \"state_event\": {\n",
    "                \"free_ball\": 1.0\n",
    "            },\n",
    "            \"end_values\": {}\n",
    "        },\n",
    "        #states_dict:\n",
    "        #---->states_dict= state_infos[key][1]\n",
    "        #---->states\n",
    "        {\n",
    "            #new_states 注意：这个时候是批次，长度为batch_size\n",
    "            \"global_state\": {\n",
    "                \"attack_remain_time\":0.0,\n",
    "                #-->27行\n",
    "            }，\n",
    "            \"self_state\": {\n",
    "                #-->41行\n",
    "            },\n",
    "            \"ally_0_state\": {}\n",
    "            \"ally_1_state\": {},\n",
    "            \"enemy_0_state\": {},\n",
    "            \"enemy_1_state\": {},\n",
    "            \"enemy_2_state\": {},\n",
    "            #global_feature = states[0].float()\n",
    "            #self_feature = states[1]\n",
    "            #ally0_feature = states[2]\n",
    "            #ally1_feature = states[3]\n",
    "            #enemy0_feature = states[4]\n",
    "            #enemy1_feature = states[5]\n",
    "            #enemy2_feature = states[6]\n",
    "            \n",
    "        [],[],... \n",
    "        #for state in states: 新增加一个轴，为批次\n",
    "        #new_states.append(state[np.newaxis, :])\n",
    "        }，\n",
    "        #action_mask:\n",
    "        #---->action_mask = np.array(state_infos[key][-1])\n",
    "        [\n",
    "            1.0,\n",
    "            1.0,\n",
    "            1.0,\n",
    "        ]\n",
    "    ],\n",
    "    \"1\":[\n",
    "        {\n",
    "\n",
    "        },\n",
    "        {\n",
    "\n",
    "        },\n",
    "        [\n",
    "\n",
    "        ]\n",
    "    ]\n",
    "}   \n",
    "```\n",
    "\n",
    "##### (env)states_dic\n",
    "```python\n",
    "#(env)states > (Policy.get_actions)states_dic\n",
    "states_dic={\n",
    "    #(Policy.get_actions)states = states_dic[key]\n",
    "    \"0\":\n",
    "        #(Policy.sample_action)states \n",
    "        #---->>>new_states[np.newaxis,:]\n",
    "        [   \n",
    "            #(forward)states:\n",
    "            [\n",
    "                #(forward)states[0].float() | \"global_state\"|global_feature: \n",
    "                {\n",
    "                    \"attack_remain_time\":0.0,\n",
    "                    \"match_remain_time\": 179.82485961914062,\n",
    "                    \"is_home_team\": 1.0,\n",
    "                    \"home_score\": 0.0,\n",
    "                    \"away_score\": 0.0,\n",
    "                    #-->27行\n",
    "                },\n",
    "                #(forward)states[1] | \"self_state\" | self_feature : \n",
    "                {\n",
    "                    \"character_id\": 1.0,\n",
    "                    \"position_type\": 3.0,\n",
    "                    \"buff_key\": 0.0,\n",
    "                    \"buff_value\": 0.0,\n",
    "                    \"stature\": 206.0,\n",
    "                    \"rational_shoot_distance\": 10.0,\n",
    "                    \"position_x\": -1.75,\n",
    "                    #-->41行\n",
    "                },\n",
    "                #(forward)states[2] | \"ally_0_state\" | ally0_feature: \n",
    "                {},\n",
    "                #(forward)states[3] | \"ally_1_state\"| ally1_feature: \n",
    "                {},\n",
    "                #(forward)states[4] | \"enemy_0_state\" | enemy0_feature: \n",
    "                {},\n",
    "                #(forward)states[5] | \"enemy_1_state\" | enemy1_feature: \n",
    "                {},\n",
    "                #(forward)states[6] | \"enemy_2_state\" | enemy2_feature: \n",
    "                {},\n",
    "#BUG:注意-在RL的时候，还有states[7]：这是动作掩码++++++++++++++++++++++++++\n",
    "            #if len(states) > 7: # action mask for rl training\n",
    "            #---------action_mask = states[7].float()\n",
    "                [0,0,...,0,1,1,0,1]，       \n",
    "#========================================================================\n",
    "            ],\n",
    "            [],\n",
    "            [],\n",
    "            #batch_size\n",
    "        ],\n",
    "    \"1\":[]\n",
    "}\n",
    "```\n",
    "##### (env)infos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (env)states\n",
    "\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 改进策略：\n",
    "1. rl环境返回的state新增一个`action_mask`作为动作掩码\n",
    "2. rl环境的(Policy.get_actions)self.log_probs = {}居然被trian越级调用，应该用return返回才对\n",
    "3. get_actions由于用的是上一个states.key()，导致每次采取动作时，最少只会有四个人的动作\n",
    "4. 策略使用IPPO，完全直接对每个智能体用一个单智能体强化学习算法来学习，但是前提是——\"同质的\"，然而这里是不满足这个条件的，也就不能粗暴共用同一套网络\n",
    "5. 缺乏转换的抽象层，值得注意的是，`take_action`和`update`均各自搞一套数据转换，缺乏效率还不统一\n",
    "\n",
    "##### 着重注意特点\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll simulate the environment and generate sample data for states, next_states, actions, rewards, and truncated.\n",
    "\n",
    "# For simplicity, I'll assume we have 3 agents (0, 1, 2). \n",
    "\n",
    "states = {\n",
    "    \"0\": [{\"global_state\": {\"attack_remain_time\": 0.5}}, {\"self_state\": {\"position_x\": -1.5}}],\n",
    "    \"1\": [{\"global_state\": {\"attack_remain_time\": 0.5}}, {\"self_state\": {\"position_x\": 1.5}}],\n",
    "    \"2\": [{\"global_state\": {\"attack_remain_time\": 0.5}}, {\"self_state\": {\"position_x\": 0.5}}],\n",
    "    \"3\": [{\"global_state\": {\"attack_remain_time\": 0.5}}, {\"self_state\": {\"position_x\": 0.0}}]  # This agent will be missing in other dicts\n",
    "}\n",
    "\n",
    "next_states = {\n",
    "    \"0\": [{\"global_state\": {\"attack_remain_time\": 0.4}}, {\"self_state\": {\"position_x\": -1.4}}],\n",
    "    \"1\": [{\"global_state\": {\"attack_remain_time\": 0.4}}, {\"self_state\": {\"position_x\": 1.4}}],\n",
    "    \"2\": [{\"global_state\": {\"attack_remain_time\": 0.4}}, {\"self_state\": {\"position_x\": 0.4}}]\n",
    "}\n",
    "\n",
    "actions = {\n",
    "    \"0\": 1,\n",
    "    \"1\": 2,\n",
    "    \"2\": 0\n",
    "}\n",
    "\n",
    "rewards = {\n",
    "    \"0\": 0.5,\n",
    "    \"1\": -0.5,\n",
    "    \"2\": 0.0\n",
    "}\n",
    "\n",
    "truncated = {\n",
    "    \"0\": False,\n",
    "    \"1\": False,\n",
    "    \"2\": True\n",
    "}\n",
    "\n",
    "# Calculate the share_keys based on the provided code\n",
    "share_keys = list(set(states.keys()) & set(next_states.keys()) & set(actions.keys()) & set(rewards.keys()) & set(truncated.keys()))\n",
    "share_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 现在有一件事可以明确，update所用的exps和\n",
    "\n",
    "##### 拆分`[\"0\"]~[\"5\"]` |(env)states >(share_keys)state\n",
    "注意\n",
    "```python\n",
    "for key in share_keys:\n",
    "    state = states[key]\n",
    "    ...\n",
    "    exp = Exp(state=state,...)\n",
    "    exps.append(exp)\n",
    "```\n",
    "##### 经验类`Exp` | (share_keys)state >(def train)exps\n",
    "只是一个简单的数据容器，比较原始，不能给神经网络使用。核心是把 字典键+值 转为 属性+值\n",
    "\n",
    "```python\n",
    "class Exp:\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n",
    "```\n",
    "##### 经验池(Policy)memory`Memory`\n",
    "在使用的时候，则又把Exp容器中的数据push到`Memory`对象中：\n",
    "```python\n",
    "if len(exps) >= 512:\n",
    "    policy.memory.push(exps)\n",
    "    policy.update(stats_recorder = stats_recorder)\n",
    "    exps = []\n",
    "```\n",
    "其中Memory则是把原始的数据，转变为给神经网络使用的数据，分别在三个层面进行：\n",
    "1. `push`简易地把Exp对象压入`self.buffer(collections.queue)`中，并用一个`maxlen`实现交换池\n",
    "2. `handle`是真正处理数据的部分，核心是转为np数组，`@return (states,next_states,actions,rewards,dones,old_log_probs)`\n",
    "3. `sample`则是简单从`buffer`中弹出一组states，并调用`handle`处理数据\n",
    "\n",
    "```python\n",
    "class Memory():\n",
    "    def __init__(self) -> None:\n",
    "        self.buffer = deque(maxlen=10000)\n",
    "    def push(self,exps):\n",
    "        self.buffer.append(exps)\n",
    "    def handle(self, exps):\n",
    "        ...\n",
    "        return states, next_states, actions, rewards, dones, old_log_probs\n",
    "    def sample(self):\n",
    "        ...\n",
    "        exps = self.buffer.popleft()\n",
    "        return self.handle(exps)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 2, 3]), tensor([4, 5]), tensor([6, 7, 8, 9])]\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设我们有3个1D张量作为old_log_probs的示例\n",
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "tensor_2 = torch.tensor([4, 5])\n",
    "tensor_3 = torch.tensor([6, 7, 8, 9])\n",
    "\n",
    "old_log_probs = [tensor_1, tensor_2, tensor_3]\n",
    "print(old_log_probs)\n",
    "\n",
    "# 使用torch.cat沿第0维拼接，然后使用unsqueeze增加一个维度\n",
    "concatenated_old_log_probs = torch.cat(old_log_probs, dim=0)\n",
    "print(concatenated_old_log_probs)\n",
    "\n",
    "concatenated_old_log_probs = concatenated_old_log_probs.unsqueeze(dim=1)\n",
    "print(concatenated_old_log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 问题：观测空间问题\n",
    "每次返回的states并不是所有人的数据，因此MADDPG并不能很好适用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4_DCD380",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
