import sys,os;sys.path.append(os.getcwd())
import torch
import torch.nn as nn
import random
from collections import deque
import numpy as np
from collections import defaultdict
from pathlib import Path
from torch.utils.tensorboard import SummaryWriter 
#from baselines.common.model import Model
from baselines.common.wrappers import RLWrapper #å…³é”®å¯¼å…¥
from DunkCityDynasty.env.gym_env import GymEnv

from pprint import pprint
from hpn_policy import *

#====================================================================================
class zmrPolicy(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        #----------------------------------------------
        # ä¸»è¦è°ƒå‚
        self.hpn_hidden_dim = 128
        self.rnn_hidden_dim = 128
        self.n_heads_input = 3
        self.n_heads_output = 3

        self.hidden_state_SamAct = None
        self.hidden_state_Eva = None
        #----------------------------------------------
        self.memory = Memory()
        self.model = HPNPolicy(self.hpn_hidden_dim, self.rnn_hidden_dim,
                               self.n_heads_input, self.n_heads_output).to('cpu')
        self.update_step = 0
    def sample_action(self, states):
        #ğŸ”´æ­¤å¤„æ˜¯æœ‰é—®é¢˜------------------------------------------------------
        new_states = []
        for state in states:
            new_states.append(state[np.newaxis, :])
        new_states = [torch.tensor(state) for state in new_states]
        #+++++++++++++++++++++++++++++++++++++
        if_print_states_shape = True
        #-------------------------
        #BUG:batch_size = states.size(0)
        #æ›´æ”¹ä¸º++++++++++++
        batch_size = len(states)
        #ğŸ”´ğŸ‘†è¿™é‡Œæˆ‘æ„Ÿè§‰è¿Ÿæ—©è¦å‡ºé—®é¢˜
        #BUG-----------------------------------
        #if if_print_states_shape is True:
        #    print(f"#1-@zmrP@sample_action@stateså½¢çŠ¶: {states.size()}")
        #æ›´æ”¹ä¸º++++++++++++++++++++++++++++++
        if if_print_states_shape is False:
            # æ‰“å°æ¯ä¸ªçŠ¶æ€çš„å½¢çŠ¶ï¼Œå› ä¸º states æ˜¯åˆ—è¡¨ï¼Œæ‰€ä»¥é€ä¸€æ‰“å°
            for idx, state in enumerate(states):
                print(f"#1-@zmrP@sample_action@states[{idx}]å½¢çŠ¶: {state.shape}")
        #ğŸ”´æ‰“å°å‡ºå½¢çŠ¶
        #BUG:-------------------åŸå› ï¼šGRUå¯ä»¥å¤„ç†None
        #BUG:self.hidden_state_SamAct = self.hidden_state_SamAct if self.hidden_state_SamAct is not None else torch.zeros(batch_size, self.rnn_hidden_dim)
        value, probs, hh = self.model(new_states,self.hidden_state_SamAct)
        self.hidden_state_SamAct = hh
        #ğŸ”´å…­ä¸ªæ™ºèƒ½ä½“å…±ç”¨ä¸€ä¸ªself.hidden_state?????
        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        dist = torch.distributions.Categorical(probs)
        action = dist.sample()
        log_probs = dist.log_prob(action)
        return action.detach().numpy().item(), log_probs
    def get_random_actions(self, states):
        action_masks = {key: np.argwhere(states[key][-1] == 1.).flatten() for key in states}
        return {key: random.choices(action_masks[key].tolist(), k=1)[0] for key in action_masks}
    def get_actions(self, states_dic):
        actions = {}
#================================
        self.log_probs = {}
        #â“è¿™é‡Œå¯ä»¥ä¿®æ”¹ä»£ç è¿›è¡Œä¼˜åŒ–
#===========ğŸŸ ğŸ‘†è¿™é‡Œå·²ç»æ˜¯æ–°å¢äº†å±æ€§self.log_probs
        #key_ll = []
        for key in states_dic.keys():
            states = states_dic[key]
            actions[key], self.log_probs[key] = self.sample_action(states)
            #ğŸ”´ğŸ‘†è¿™é‡Œä¸èƒ½å› ä¸ºä¸Šä¸€ä¸ªstatesé‡Œæ²¡æœ‰åŠ¨ä½œï¼Œå°±ä¸åšåŠ¨ä½œå•Š
            #key_ll.append(key)
        #print(key_ll)
        return actions
    def _compute_returns(self, rewards,dones,gamma=0.99):
        returns = []
        R = 0
        for r, d in zip(reversed(rewards), reversed(dones)):
            if d:
                R = 0
            R = r + gamma * R
            returns.insert(0, R)
        returns = torch.tensor(np.array(returns)).unsqueeze(dim=1)
        returns = (returns - returns.mean()) / (returns.std() + 1e-9)
        return returns

    #def save_model(self, path):
        #Path(path).mkdir(parents=True, exist_ok=True)
        #torch.save(self.model.state_dict(), f"{path}/{step}")
        #torch.save(self.model.state_dict(), f"{path}")
    def evaluate(self, states, actions):
        #++++++++++++++++++++++++++++++++++++++++++++++++++++
        #BUG:-------------------åŸå› ï¼šGRUå¯ä»¥å¤„ç†None
        #self.hidden_state_Eva = self.hidden_state_Eva if self.hidden_state_Eva is not None else torch.zeros(batch_size, self.rnn_hidden_dim)
        value, probs, hh = self.model(states,self.hidden_state_Eva)
        self.hidden_state_Eva = hh
        #^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        dist = torch.distributions.Categorical(probs)
        log_probs = dist.log_prob(actions.squeeze(dim=1))
        entropys = dist.entropy()
        return value, log_probs, entropys
    def sgd_iter(self, states, actions, returns, old_log_probs):
        '''ä½¿ç”¨ sgd_iter æ–¹æ³•å¯¹æ•°æ®è¿›è¡Œéšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£.
        
        è¿™å®è´¨ä¸Šæ˜¯æŠŠæ•´ä¸ªæ•°æ®é›†åˆ†æˆå°æ‰¹æ¬¡ï¼Œå¹¶å¯¹æ¯æ‰¹æ•°æ®è¿›è¡Œæ›´æ–°ã€‚
        '''
        batch_size = actions.shape[0]
        mini_batch_size = 32
        for _ in range(batch_size//mini_batch_size):
            rand_ids = np.random.randint(0, batch_size, mini_batch_size)
            yield [states[i][rand_ids,:] for i in range(8)], actions[rand_ids,:], returns[rand_ids,:], old_log_probs[rand_ids,:]
    def update(self, *,stats_recorder=None,all_train_step):
        states, next_states, actions, rewards, dones, old_log_probs = self.memory.sample()
        old_log_probs = torch.cat(old_log_probs,dim=0).unsqueeze(dim=1)
        #ğŸŸ è¿·æƒ‘æ€§ä»£ç ğŸ‘†:ç”¨æ¥ç»™å†…å±‚åŠ æ‹¬å·çš„-->ä½œç”¨:
        #old_log_probs = [tensor([1, 2, 3]), tensor([4, 5]), tensor([6, 7, 8, 9])]
        #torch.cat(old_log_probs,dim=0) = tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
        #.unsqueeze(dim=1) = ------>æ•ˆæœç­‰ä»·äºğŸŸ¢.view(-1,1)
        #tensor([[1],
        #[2],
        #[3],
        #[4],
        #[5],
        #[6],
        #[7],
        #[8],
        #[9]])
        #BUGğŸŸ¢æ”¹è¿›æ–¹æ³•ï¼š
        if states is None: 
            return
        states = [torch.tensor(np.array(state),dtype=torch.float32) for state in states]
        next_states = [torch.tensor(np.array(next_state),dtype=torch.float32) for next_state in next_states]
        actions = torch.tensor(actions,dtype=torch.float32).unsqueeze(1)
        rewards = torch.tensor(np.array(rewards),dtype=torch.float32)
        dones = torch.tensor(np.array(dones),dtype=torch.float32)
        returns = self._compute_returns(rewards, dones)
        for _ in range(2):
            for states, actions, returns, old_log_probs in self.sgd_iter(states, actions, returns, old_log_probs):
                values, log_probs, entropys = self.evaluate(states, actions)
                advantages = returns - values.detach()
                ratio = torch.exp(log_probs.unsqueeze(dim=1) - old_log_probs.detach())
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1 - 0.2, 1 + 0.2) * advantages
                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropys.mean()
                critic_loss = nn.MSELoss()(returns,values)
                tot_loss = actor_loss + 0.5 * critic_loss
                self.model.opt.zero_grad()
                tot_loss.backward()
                self.model.opt.step()
                if stats_recorder is not None:
                    self.update_step += 1
                    all_train_step+=1
                    #stats_recorder.add_policy_loss(tot_loss.item(), self.update_step)
                    stats_recorder.add_policy_loss(tot_loss.item(),all_train_step)
                    #stats_recorder.add_value_loss(critic_loss.item(),  self.update_step)
                    stats_recorder.add_value_loss(critic_loss.item(),all_train_step)
                    #if self.update_step % 100 == 0:
                        #self.save_model('./output/bc_model', self.update_step)
                    #ğŸ‘†ä¿å­˜æ¨¡å‹ç§»åŠ¨åˆ°trainå¾ªç¯ï¼Œä¼ å…¥all_train_stepè¿›è¡Œæ§åˆ¶

class Exp:
    '''ç®€å•çš„æ•°æ®å®¹å™¨,ç”¨äºå­˜å‚¨ä¸€æ¬¡ç¯å¢ƒäº¤äº’çš„ç»“æœ.


    '''
    def __init__(self, **kwargs) -> None:
        for k,v in kwargs.items():
            setattr(self,k,v)

class Memory():
    def __init__(self) -> None:
        self.buffer = deque(maxlen=10000)
    def push(self,exps):
        self.buffer.append(exps)
        #ğŸŸ expsåº”è¯¥æ˜¯ä¸€ä¸ªç±»ğŸ‘†åœ¨æå–ç›¸åº”çŠ¶æ€æ—¶ï¼Œè¦ç”¨ï¼›
            #.state .action .reward .truncated .log_probs
    def handle(self, exps):
        '''æå–å¹¶å¤„ç†ä¿¡æ¯.
        
        ä»ç¯å¢ƒæ¥çš„ç»éªŒåˆ—è¡¨ï¼Œæå–å¹¶å¤„ç†ä¸ºèƒ½ç»™ç¥ç»ç½‘ç»œçš„ä¿¡æ¯
        '''
        states = [[] for _ in range(8)] 
        #ğŸ”µğŸ‘†æ³¨æ„state/next_stateæ˜¯åŒ…å«8ä¸ªçŠ¶æ€çš„åˆ—è¡¨state_action_mask
        for exp in exps:
            for i in range(8):
                states[i].append(exp.state[i])
        for i in range(8):
            states[i] = np.array(states[i])
            #å…¶å®å°±æ˜¯è½¬npæ•°ç»„ï¼Œä½†æ˜¯npæ•°ç»„è¦æ±‚å…¶ä¸­æ¯ä¸€ä¸ªåˆ—è¡¨éƒ½å¿…é¡»ç›¸åŒï¼Œå› æ­¤åªèƒ½éƒ¨åˆ†è½¬
        
        next_states = [[] for _ in range(8)]
        for exp in exps:
            for i in range(8):
                next_states[i].append(exp.next_state[i])
        #ğŸ”´â“æ˜¯å¦ç¼ºå°‘ next_states[i] = np.array(next_states[i]) ?
        actions = np.array([exp.action for exp in exps])
        rewards = np.array([exp.reward for exp in exps])
        dones = np.array([exp.truncated for exp in exps])
        old_log_probs =  [exp.log_probs for exp in exps]
        #--------------ğŸŸ ğŸ‘†æ ¹æœ¬å·®å¼‚çš„æ¥æº
        return states, next_states, actions, rewards, dones, old_log_probs
    def sample(self):
        if len(self.buffer) == 0: 
            return None
        exps = self.buffer.popleft()
        return self.handle(exps)

def create_env(id=1):
    env_config = {
        'id': id,
        'env_setting': 'win',
        'client_path': 'D:\\1_GitProject\\game_package_release',
        'rl_server_ip': '127.0.0.1',
        'rl_server_port': 6666,
        'game_server_ip': '47.111.110.225',
        'game_server_port': 18001,
        'machine_server_ip': '',
        'machine_server_port': 0,
        "user_name": "qmxxpmz02wf6e",
        'render': False,
    }
    wrapper = RLWrapper({})
    env = GymEnv(env_config, wrapper=wrapper)
    return env

class ZmrRecorder:
    def __init__(self,*,version) -> None:
        self.writers = {}
        self.writer_types = ['Play','Policy']
        for writter_type in self.writer_types: 
            self.writers[writter_type] = SummaryWriter(f'./tmp/hpn_v{version}_{writter_type}',flush_secs=5)
                                                                #ğŸ‘†è¿™é‡Œç‰ˆæœ¬æ³¨æ„ä¿®æ”¹
    
    def add_scalar(self,w_type,*,tag,scalar_value,global_step): 
        self.writers[w_type].add_scalar(tag=tag, scalar_value=scalar_value, global_step = global_step)
    
    def add_rewards(self, rewards,all_ep_step): #ğŸŸ¢æ³¨æ„:all_ep_stepåº”è¯¥ä¸ºcnt*n+step
        for key in rewards.keys():
            self.add_scalar('Play',tag=f'rewards/reward_{key}',scalar_value=rewards[key],global_step=all_ep_step)
    def add_policy_loss(self, loss,all_ep_step):
        self.add_scalar('Policy',tag=f'loss/policy_loss',scalar_value=loss,global_step=all_ep_step)
    def add_value_loss(self, loss, all_ep_step):
        self.add_scalar('Policy',tag=f'loss/value_loss',scalar_value=loss,global_step=all_ep_step)
    def close(self):
        for writer in self.writters.values():
            writer.close()

#&&&&&æ˜¯å¦æ­£å¼è¿è¡Œ
is_play = False
#&&&&&
def v2_train(*,env, policy,stats_recorder=None,model_version):
    model_path = f'./tmp/model/hpn_v{model_version}_model'
    #ğŸ‘†åˆå¹¶ä¿å­˜ä½ç½®^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    if os.path.exists(model_path):
        model_parameters = torch.load(model_path)
        policy.model.load_state_dict(model_parameters)
    else:
        print(f"Error:æƒé‡æ–‡ä»¶ä¸å­˜åœ¨{model_path}")
    if is_play: 
        policy.model.eval()
    ep_cnt = 0
    all_ep_step= 0 
    all_train_step = 0
    if_tmp = 0
    key_l = {0:0,1:0,2:0,3:0,4:0,5:0}
    for i in range(1000000):
        exps = []
        states, infos = env.reset()
        ep_rewards = defaultdict(int)
        ep_step = 0
        while True:
            actions = policy.get_actions(states)
            #if ep_cnt < 50:
            #    actions = policy.get_random_actions(states)
            next_states , rewards, dones, truncated, infos = env.step(actions)
            ep_step += 1
            all_ep_step +=1
            share_keys = list(set(states.keys()) & set(next_states.keys()) & set(actions.keys()) & set(rewards.keys()) & set(truncated.keys()))
            #ğŸŸ ğŸ‘†è¿™ä¸ªshare_keyså’Œä¸‹é¢çš„for inå¾ªç¯æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ
            #è¿”å›çš„æ˜¯å…·æœ‰å®Œæ•´ä¿¡æ¯çš„['0', '1', '2']
            #ç¡®ä¿æˆ‘ä»¬å¤„ç†çš„æ¯ä¸ªæ™ºèƒ½ä½“åœ¨å½“å‰æ—¶é—´æ­¥éƒ½æœ‰å®Œæ•´çš„ä¿¡æ¯
            #æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›å¤„ç†é‚£äº›åœ¨æŸäº›å­—å…¸ä¸­ç¼ºå¤±æ•°æ®çš„æ™ºèƒ½ä½“
            #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
            
            if_use_states_key = False

            if if_use_states_key:
                for key in states.keys():
                    try:
                        if states[key][0][14]== float(2):
                            print('Ball fallen========= ({0},{1},{2})'.format(states[key][0][5],states[key][0][6],states[key][0][7]))
                    except:
                        continue
                    key_l[key]+=1
                if all_ep_step%50==0:
                    #print(np.array(key_l.values)/sum(key_l.values))
                    print(np.array(list(key_l.values())) / sum(key_l.values()))
                if if_tmp == 0:
                    pprint(states[key])
                    print("--------------------------")
            if_tmp = 1
            #pprint(rewards)
            #pprint(infos)
            #pprint(rewards)
            #pprint(actions)
            #print("-----------------------------------------")
            #=====================================================================================================
            for key in rewards.keys():
                ep_rewards[key] += rewards[key]
            if truncated['__all__'] or ep_step >= 120:
                ep_step = 0
                ep_cnt += 1
                #stats_recorder.add_rewards(ep_rewards, ep_cnt)
            stats_recorder.add_rewards(rewards,all_ep_step)

            for key in share_keys:
                state = states[key]
                next_state = next_states[key]
                action = actions[key]
                reward = rewards[key]
                done = truncated[key]
                truncat = truncated[key]
    #========================================================================
                log_probs = policy.log_probs[key]
                #(Policy.get_actions)self.log_probs = {}
                #æ¯æ¬¡é€‰æ‹©åŠ¨ä½œåï¼Œéƒ½ä¼šé‡ç½®
    #==================================ğŸŸ ğŸ‘†æ³¨æ„è¿™ä¸ªlog_probsæ˜¯ä»policyé‡Œå‡ºæ¥çš„
                exp = Exp(state=state,next_state=next_state,action=action,reward=reward,done=done,log_probs=log_probs,truncated=truncat)
                exps.append(exp)
            if len(exps) >= 512:
                policy.memory.push(exps)
                #policy.update(stats_recorder = stats_recorder,all_train_step=all_train_step)
                exps = []
                #++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                print(f'{all_train_step}SAVE_MODEL+++++++++++++++++++++++++++')
                #policy.save_model(f'./tmp/model/hpn_v{model_version}_model')
                #torch.save(policy.model.state_dict(), f'./tmp/model/hpn_v{model_version}_model')
                #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
                # ç¡®ä¿è·¯å¾„å­˜åœ¨
                os.makedirs(os.path.dirname(model_path), exist_ok=True)
                # ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸
                torch.save(policy.model.state_dict(), model_path)

            if dones['__all__']:
                break
            states = next_states


def v2_main(*,version,model_version):
    env = create_env()
    policy = zmrPolicy()
    stats_recorder = ZmrRecorder(version=version)
    v2_train(env=env,policy=policy,stats_recorder=stats_recorder,model_version=model_version)
    stats_recorder.close()

if __name__ == '__main__':
    v2_main(version=1,model_version=1)